#!/usr/bin/env python3
"""
Test the enhanced chat UX structure to confirm it matches user requirements
"""
import sys
import os

# Add pandasai to path
sys.path.insert(0, 'pandasai')

def test_enhanced_ux_structure():
    """Test that the enhanced result follows the correct UX structure"""
    print("ğŸ¯ Testing Enhanced Chat UX Structure")
    print("="*40)

    try:
        from pandasai.pipelines.chat.market_commentary import MarketCommentary
        from unittest.mock import Mock

        # Create mock context and logger
        mock_context = Mock()
        mock_context.config = Mock()
        mock_context.config.use_harmony_format = True
        mock_context.config.enable_market_commentary = True
        mock_context.config.enable_next_steps_prompt = True
        mock_context.config.llm = Mock()

        # Mock LLM responses
        mock_context.config.llm.chat_completion = Mock(side_effect=[
            "The analysis shows a strong upward trend in sales with an average of $1,245 per transaction. This represents solid performance in the current market conditions.",
            "1. Compare with previous quarter's performance\n2. Analyze by customer segment\n3. Review seasonal patterns\n4. Identify top-performing products"
        ])

        mock_context.get = Mock(side_effect=lambda key, default="": {
            'current_user_query': 'What is the average sales amount?',
            'last_code_generated': 'result = {"type": "number", "value": f"{df[\'sales\'].mean():.2f}"}'
        }.get(key, default))

        mock_logger = Mock()

        # Create market commentary step
        commentary_step = MarketCommentary()

        # Test with a sample executed result (simulates materialized f-string values)
        executed_input = {
            "type": "number",
            "value": 1245.67  # This is the materialized value from the f-string
        }

        # Execute the enhanced pipeline step
        result = commentary_step.execute(
            executed_input,
            context=mock_context,
            logger=mock_logger
        )

        print("âœ… Pipeline execution successful")

        # Verify the enhanced UX structure
        enhanced_result = result.output

        print("\nğŸ“Š Enhanced Result Structure:")
        print(f"  Enhanced chat response: {enhanced_result.get('enhanced_chat_response', False)}")

        # Check primary result (artifacts)
        primary = enhanced_result.get('primary_result', {})
        print(f"\nğŸ¯ Primary Artifacts:")
        print(f"  Type: {primary.get('type')}")
        print(f"  Value: {primary.get('value')}")

        # Check market commentary (conversationalized from executed results)
        commentary = enhanced_result.get('market_commentary', {})
        print(f"\nğŸ’¼ Market Commentary:")
        print(f"  Generated from: {commentary.get('generated_from')}")
        print(f"  Content preview: {commentary.get('content', '')[:100]}...")

        # Check next steps (contextual follow-up prompts)
        next_steps = enhanced_result.get('next_steps', {})
        print(f"\nğŸ”® Next Steps:")
        print(f"  Enabled: {next_steps.get('enabled')}")
        print(f"  Prompt style: {next_steps.get('prompt_style')}")
        print(f"  Suggestions preview: {next_steps.get('suggestions', '')[:100]}...")

        # Verify structure matches requirements
        structure_checks = {
            "Has primary artifacts": 'primary_result' in enhanced_result,
            "Has market commentary": 'market_commentary' in enhanced_result,
            "Has next steps": 'next_steps' in enhanced_result,
            "Commentary from executed results": commentary.get('generated_from') == 'executed_result_analysis',
            "Next steps enabled": next_steps.get('enabled', False),
            "Enhanced chat flag": enhanced_result.get('enhanced_chat_response', False)
        }

        print(f"\nâœ… Structure Validation:")
        all_passed = True
        for check, passed in structure_checks.items():
            status = "âœ…" if passed else "âŒ"
            print(f"  {status} {check}")
            if not passed:
                all_passed = False

        if all_passed:
            print(f"\nğŸ‰ Perfect! Enhanced UX structure matches requirements:")
            print(f"  1. âœ… Primary artifacts (plots, data, etc.) - Separated")
            print(f"  2. âœ… Market commentary (from materialized f-string results) - Generated")
            print(f"  3. âœ… Next steps suggestions (contextual follow-ups) - Enabled")
            print(f"\nğŸ”— This matches the industry-standard chat UX you requested!")
            return True
        else:
            print(f"\nâš ï¸  Some structure requirements not met")
            return False

    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_fallback_functionality():
    """Test that fallback works when LLM is unavailable"""
    print("\nğŸ›¡ï¸  Testing Fallback Functionality (LLM unavailable)")
    print("="*50)

    try:
        from pandasai.pipelines.chat.market_commentary import MarketCommentary
        from unittest.mock import Mock

        # Create mock context with failing LLM
        mock_context = Mock()
        mock_context.config = Mock()
        mock_context.config.use_harmony_format = True
        mock_context.config.enable_market_commentary = True
        mock_context.config.enable_next_steps_prompt = True
        mock_context.config.llm = Mock()

        # Make LLM fail
        mock_context.config.llm.chat_completion = Mock(side_effect=Exception("LLM unavailable"))

        mock_context.get = Mock(side_effect=lambda key, default="": {
            'current_user_query': 'Calculate total revenue',
            'last_code_generated': 'result = {"type": "number", "value": f"{df[\'revenue\'].sum():.2f}"}'
        }.get(key, default))

        mock_logger = Mock()

        # Test with executed result
        executed_input = {
            "type": "number",
            "value": 15420.50  # Materialized f-string result
        }

        commentary_step = MarketCommentary()
        result = commentary_step.execute(
            executed_input,
            context=mock_context,
            logger=mock_logger
        )

        enhanced_result = result.output
        commentary = enhanced_result.get('market_commentary', {})
        next_steps = enhanced_result.get('next_steps', {})

        print("âœ… Fallback execution successful")
        print(f"ğŸ“Š Fallback commentary includes actual value: {'15420.50' in commentary.get('content', '')}")
        print(f"ğŸ”® Fallback next steps generated: {bool(next_steps.get('suggestions'))}")

        return True

    except Exception as e:
        print(f"âŒ Fallback test failed: {e}")
        return False

def main():
    """Run enhanced UX tests"""
    print("ğŸš€ Enhanced Chat System - UX Structure Tests")
    print("="*50)

    results = []
    results.append(("Enhanced UX Structure", test_enhanced_ux_structure()))
    results.append(("Fallback Functionality", test_fallback_functionality()))

    print("\nğŸ“Š Test Results Summary:")
    print("="*25)

    passed = 0
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {test_name:<25} {status}")
        if result:
            passed += 1

    print(f"\nğŸ¯ Overall: {passed}/{len(results)} tests passed")

    if passed == len(results):
        print("\nğŸ‰ Enhanced Chat UX is ready for production!")
        print("\nğŸ“‹ Per User Query Response Structure:")
        print("  1. ğŸ¯ Primary Artifacts - Data, plots, calculations")
        print("  2. ğŸ’¼ Market Commentary - Conversationalized from actual f-string results")
        print("  3. ğŸ”® Next Steps - \"Do you want me to...\" contextual suggestions")
        print("\nâœ¨ All values are materialized from executed f-string code!")
        print("ğŸ”— Industry-standard chat UX achieved!")
    else:
        print("\nâš ï¸  Some components need attention")

if __name__ == "__main__":
    main()